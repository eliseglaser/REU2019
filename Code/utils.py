import gensim, logging, os, csv, glob, re, pickle
import pandas as pd
from sklearn.cluster import KMeans
import numpy as np
import datetime
#import function for normalization
from sklearn.preprocessing import normalize

'''
This function takes a string and a list of stop words to remove.
The function returns the string with the stopwords removed
'''
def stopless_text_generator(s, stop_words):
    tokens = s.split()
    stopless_tokens = [tok for tok in tokens if tok not in stop_words]
    return " ".join(stopless_tokens)
    
'''
This function calculates the weeks between given epoch and Jan 1 of a baseyear.
Weeks are calculated by counting the weeks between the Monday's of each date given.
'''
def weeks_since(epoch, base):
    d1 = datetime.datetime(base,1,1)
    d2 = datetime.datetime.utcfromtimestamp(epoch)
    mon1 = (d1 - datetime.timedelta(days=d1.weekday()))
    mon2 = (d2 - datetime.timedelta(days=d2.weekday()))
    
    return ((mon2 - mon1).days / 7)

'''
This function calculates the months between given epoch and Jan 1 of a baseyear
'''
def months_since(epoch, base):
    d1 = datetime.datetime(base,1,1)
    d2 = datetime.datetime.utcfromtimestamp(epoch)
    
    diff = (d2.year - d1.year) * 12 + d2.month - d1.month
    return diff


""" Handle reading and parsing data"""



"""

Helper function for reading from a directory of CSVs
dirname: the name of the directory of CSVs
key: The name of the variable to extract from the CSV
returns a generator that returns the next string in the file
"""
def read(dirname,keys):
        for fname in os.listdir(dirname):
            first = True
            locs =[]
            with open(dirname+ "/"+fname,'r') as csvfile:                
                reader = csv.reader(csvfile,dialect='excel',delimiter=',',quotechar='\"')
                try:
                    for row in reader :
                        # get the location the variable to extract
                        if first:
                            for key in keys:
                                locs.append(row.index(key))
                            first = False
                        else:
                            sentence =""
                            for loc in locs:
                                sentence=sentence+" "+row[loc]
                            yield sentence             
                except:
                    print (fname + " has an error")
                csvfile.close()

"""
  Helper function for reading from a directory of CSVs into a pandas dataframe    dirname: the name of the directory of CSVs
"""
def read_df(dirname, extension = "/*.csv"):
    frame = pd.DataFrame()
    df_list =[]
    fnames = glob.glob(dirname + extension)
    for fname in fnames:
        try:
            df = pd.read_csv(fname,header=0)
            df_list.append(df)
        except:
            print ("Couldn't read :", fname)
    frame = pd.concat(df_list)
    return frame
        

# Takes a string and returns a cleaned version                
def clean_sentence(sentence):
    # remove case
    sentence = sentence.lower()
    # remove special characters
    exclude = "[•…“”\_\-,.;:\)\(\[\]0123456789/?&#%+@\\\=\*$\"!\r~\n\^]"
    temp = re.sub(exclude," ",sentence)
    return re.sub("[^a-z\ ]","",temp)
"""
    Takes a string and returns a version where all links have been
    replaced with the word link
"""
def remove_links(sentence):
    pattern="http://[^ \n\r]*"
    return re.sub(pattern," link ",sentence)
"""
    Removes any comments that are not a direct response to the submission
    Replace NaN values for posts so they don't get deleted
"""
def remove_comments(df):
    df["link_id"].fillna(0)
    df["parent_id"].fillna(0)
    return df[df["link_id"] == df["parent_id"]]

"""
    Changes comment link_id to be in the same format as parent id
"""
def clean_id(id):
    return id[3:]


"""
Save and load utility files
Taken in part from:
https://stackoverflow.com/questions/19201290/how-to-save-a-dictionary-to-a-file
"""
def save_object(obj,dir, name ):
    with open(dir + name + '.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_object(dir,name):
    with open(dir + name + '.pkl', 'rb') as f:
        return pickle.load(f)


    """" Analyze Cluster functions"""

"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of word vectors
and uses this to generate a list of clusters formated to contain the number
of unique words in the cluster, the total frequency of those words in the
corpus, and a list of all words, with their associate frequencies
"""
def make_clustering_objects(word2vecModel,kmeans,vocab_list,WordByFeatureMat):
    clusters =[]
    for i in range(len(kmeans.cluster_centers_)):
        clusters.append( {'unique_words':0,'total_freq':0,'word_list':[]})
    
    predictions = kmeans.predict(WordByFeatureMat)
    for i in range(len(vocab_list)):
        cluster   = predictions[i]
        word      = vocab_list[i]
        freq      = word2vecModel.wv.vocab[word].count
        clusters[cluster]['unique_words'] += 1
        clusters[cluster]['total_freq'] += freq
        clusters[cluster]['word_list'].append((word,freq))
    return clusters


"""
Make a dictionary of the words and frequencies of the words in a model.
"""
def make_frequency_dictionary(word2vecModel):
    vocab_list = sorted(list(word2vecModel.wv.vocab))
    word_freq = {}
    for i in range(len(vocab_list)):
        word = vocab_list[i]
        freq = word2vecModel.wv.vocab[word].count
        word_freq.update({word:freq})
    return word_freq

"""
Make cluster favorances normalized that can be used to create the bar graph. 
Input is a list of subreddit abbreviations and the model name used in 
BuildModelsAndMatrices.
"""
def generate_cluster_favorance(model_name,subreddits):
    #Load Matricies
    matricies = []
    for sub in subreddits:
        matricies.append(load_object('matricies/',model_name + '-' + sub + 'PostsByClusters-100clusters'))
    #Normalize Rows
    matriciesNormed = []
    for matrix in matricies:
        matriciesNormed.append(normalize(matrix, axis=1, norm='l1'))
    #Sum all rows.
    summedMatrix = []
    for matrixNorm in matriciesNormed:
        summedMatrix.append(matrixNorm.sum(axis=0))
    #Normalize Sum
    sumNormMatrix = []
    for matrixSum in summedMatrix:
        sumNormMatrix.append(normalize(matrixSum.reshape(1,-1),axis=1,norm='l1'))
    return sumNormMatrix


"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of word vectors
and uses this to generate a list of clusters formated to contain the number
of unique words in the cluster, the term-frequency inverse document frequency of those words in the
corpus, and a list of all words, with their associate frequencies
"""
def make_clustering_objects_tfidf(word2vecModel,kmeans,vocab_list,tfidf_list,WordByFeatureMat):
    clusters =[]
    for i in range(len(kmeans.cluster_centers_)):
        clusters.append( {'unique_words':0,'total_freq':0,'word_list':[]})
    
    predictions = kmeans.predict(WordByFeatureMat)
    for i in range(len(vocab_list)):
        cluster   = predictions[i]
        word      = vocab_list[i]
        freq      = word2vecModel.wv.vocab[word].count
        tfidf     = tfidf_list[i]
        clusters[cluster]['unique_words'] += 1
        clusters[cluster]['total_freq'] += freq
        clusters[cluster]['word_list'].append((word,tfidf))
    return clusters


    
"""
Make Clustering Objects
Takes the kmeans object generated by sklearn, the number of clusters, and the
matrix of posts vectors
and uses this to generate a list of clusters formated to contain the number
 of posts in this cluster, the mean score, the median score, and the range
"""
def make_post_clusters(kmeans,PostsByXMat,scores,num_comments_list):
    clusters =[]
    num_post_clusters = len(kmeans.cluster_centers_)
    num_features = len(kmeans.cluster_centers_[0])
    for i in range(num_post_clusters):
        clusters.append( {'total_posts':0,'post_list':[],
                          'center':list(zip(kmeans.cluster_centers_[i].tolist(),range(1,1+num_features)))})
    predictions = kmeans.predict(PostsByXMat)
    posts_vecs = PostsByXMat.tolist()
    for i in range(len(posts_vecs)):
        cluster = predictions[i]
        clusters[cluster]['total_posts'] +=1
        clusters[cluster]['post_list'].append({'score':scores[i], 'num_comments':num_comments_list[i],
                                              'vector':list(zip(posts_vecs[i],range(1,1+num_features)))})
    for cluster in clusters:
        cluster['post_list'].sort(key = (lambda x: x['score']))
        total_score=0
        total_comments=0
        min_score    = cluster['post_list'][0]['score']
        max_score    = cluster['post_list'][-1]['score']
        cluster['score_range'] = max_score-min_score
        midpoint=len(cluster['post_list'])//2
        cluster['score_median']  = cluster['post_list'][midpoint]['score']
        cluster['post_list'].sort(key = (lambda x: x['num_comments']))
        min_comments = cluster['post_list'][0]['num_comments']
        max_comments = cluster['post_list'][-1]['num_comments']
        cluster['comments_range'] = max_comments-min_comments
        for post in cluster['post_list']:
            total_score += post['score']
            total_comments += post['num_comments']
        cluster['score_mean']= total_score/cluster['total_posts']
        cluster['comments_mean'] = total_comments/cluster['total_posts']
        cluster['comments_median']  = cluster['post_list'][midpoint]['num_comments']
    return clusters

'''
NEED COMMENT FOR FUNCTION
'''
    
'''
Code to quote around text with certain key words.
'''
def quoteText(text,clusterNumber,model_name = "DrugAbuse",numberofcluster=100):
    model = gensim.models.Word2Vec.load('models/' + model_name + '.model')
    WordsByFeatures = load_object('matricies/', model_name + "-WordsByFeatures")
    vocab_list = sorted(list(model.wv.vocab))

    # Initialize a word clustering to use
    num_word_clusters = numberofcluster
    kmeans =  load_object('clusters/', model_name + '-words-cluster_model-' + str(num_word_clusters))
    clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)
    clusterWords = list(map(lambda x: list(map(lambda y: y[0] , x["word_list"])), clusters))
    keyArrayOG = clusterWords[clusterNumber-1]
    #Get Bigrams into nice pattern
    keyArray = []
    for word in keyArrayOG:
        newWord = ''
        split = word.split('_')
        if len(split) > 1:
            for i in range(len(split)):
                if i != (len(split)-1):
                    newWord += split[i] + ' '
                else:
                    newWord += split[i]
        else:
            keyArray.append(word)
            continue
        keyArray.append(newWord)
    #Start looking for keywords.
    for keyWord in keyArray:
        a = re.split('(?<= )' + keyWord + '(?=[,?!. ])(?i)',text)
        final = ''
        index = 0
        for word in a:
            if index != len(a) - 1:
                final += word + '"' + keyWord + '"'
            else:
                final += word 
            index += 1
        text = final
    print(final)
'''
Gets the top X number of words in a word model operating on the assumption that you've already made 
the Word2Vec model using the BuildModelsAndMatricies notebook.
'''
def get_top_words(clusterNumber,numWords,model_name = "DrugAbuse",numClusters = 100):
    #Create Vocab
    num_clusters = numClusters
    model = gensim.models.Word2Vec.load('models/' + model_name + '.model')
    kmeans = load_object('clusters/', model_name + "-words-cluster_model-" + str(num_clusters))
    WordsByFeatures = load_object('matricies/', model_name + '-' + 'WordsByFeatures')
    vocab_list = sorted(list(model.wv.vocab))
    clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)
    #Sort by word frequency
    vocab_table = []
    for i in range(len(clusters)):
        listUnsorted = clusters[i]['word_list']
        vocab_sorted = sorted(listUnsorted, key=lambda x: -x[1])
        vocab_table.append(vocab_sorted)
    returnString = ''
    for i in range(numWords):
        if i != numWords-1:
            returnString += '"' + vocab_table[clusterNumber-1][i][0] + '", '
        else: 
            returnString += 'and "' + vocab_table[clusterNumber-1][i][0] + '"'
    print(returnString)
'''
Take in an array of cluster numbers and the number of words and output an array that can be used 
to make a csv file for easy table writing.
'''
def make_csv_top_words(model_name, clusterNumbers, numWords, nameCSV = 'NiceClusters.csv'):
    clusterWords = []
    for i in range(len(clusterNumbers)):
        clusterW = get_top_words_csvStyle(clusterNumbers[i],numWords,model_name)
        clusterWArray = clusterW.split(', ')
        clusterWords.append(clusterWArray)
    #Get n word in n row for print format.
    printRows = []
    clusterHeadings = []
    for i in range(len(clusterNumbers)):
        clusterHeadings.append('Cluster ' + str(clusterNumbers[i]))
    printRows.append(clusterHeadings)
    for i in range(numWords):
        array = []
        for j in range(len(clusterWords)):
            array.append(clusterWords[j][i])
        printRows.append(array)
    #Write to a CSV File
    import csv
    import pandas as pd
    with open(model_name + nameCSV + '.csv','w') as csvfile:
        csvwriter = csv.writer(csvfile)
        for i in range(len(printRows)):
            csvwriter.writerow(printRows[i])
            i = i + 1
    csvfile.close()
'''
Return top words from a cluster number in format that's useful for making a csv file.
This function is similar to get_top_words but is used in make_csv_top_words.
'''
def get_top_words_csvStyle(clusterNumber,numWords,model_name,numClusters = 100):
    #Create Vocab
    num_clusters = numClusters
    model = gensim.models.Word2Vec.load('models/' + model_name + '.model')
    kmeans = load_object('clusters/', model_name + "-words-cluster_model-" + str(num_clusters))
    WordsByFeatures = load_object('matricies/', model_name + '-' + 'WordsByFeatures')
    vocab_list = sorted(list(model.wv.vocab))
    clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)
    #Sort by word frequency
    vocab_table = []
    for i in range(len(clusters)):
        listUnsorted = clusters[i]['word_list']
        vocab_sorted = sorted(listUnsorted, key=lambda x: -x[1])
        vocab_table.append(vocab_sorted)
    returnString = ''
    for i in range(numWords):
        if i != numWords-1:
            returnString +=  vocab_table[clusterNumber-1][i][0] + ', '
        else: 
            returnString += vocab_table[clusterNumber-1][i][0]
    return returnString

#Get Support values for specific rules and update the itemsets dictionaries. Used in Association-analysis-Multiple Subreddits.
def get_support_2_cluster(array,PostsByClusters,itemsets):
    index1 = 0
    for itemset in itemsets:
        supportcount = 0
        try:
            itemset[frozenset({(array[0]),(array[1])})]
            index1 = index1 + 1
            continue
        except KeyError:
            for post in PostsByClusters[index1]:
                if post[(0,(array[0]))] != 0 and post[(0,(array[1]))] != 0:
                    supportcount += 1
            itemset[frozenset({(array[0]),(array[1])})] = supportcount
            supportcount = 0
            index1 = index1 + 1
    return itemsets
    
#Same as get_support_2_cluster but with only one cluster. Used in Association-analysis-Multiple Subreddits.
def get_support_1_cluster(array,PostsByClusters,itemsets):
    index1 = 0
    for itemset in itemsets:
        supportcount = 0
        try:
            itemset[frozenset({(array[0])})]
            index1 = index1 + 1
            continue
        except KeyError:
            for post in PostsByClusters[index1]:
                if post[(0,(array[0]))] != 0:
                        supportcount += 1
            itemset[frozenset({(array[0])})] = supportcount
            supportcount = 0
            index1 = index1 + 1
    return itemsets

#Get all top rules to analyze
def get_rule_pairs(rules_slim):
    all_rule_numbers = []
    for rules in rules_slim:
        subreddit_rules = len(rules)
        i = 0
        rule_numbers = []
        frozen_numbers = []
        while i < subreddit_rules:
            test = [list(x) for x in rules]
            frozen_numbers.append((test[i][0],test[i][1]))
            rule_numbers.append(tuple((list(x)[0] for x in frozen_numbers[i])))
            i = i + 1
        all_rule_numbers.append(rule_numbers)
    return all_rule_numbers
