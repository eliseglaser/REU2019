{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Models\n",
    "This notebook will walk you through building and saving the most basic \n",
    "models and matricies we used for analyzing our text data. \n",
    "This notebook allows you to build word2vec models and produce the following matrices for multiple subreddits.\n",
    "The model itself is built on a corpus of all the data in the data folder so clusters are consistent across subreddit.\n",
    "\n",
    "We first import the libraries and utility files we are going to be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/eliseglaser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import useful mathematical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import useful Machine learning libraries\n",
    "import gensim\n",
    "\n",
    "#Import nltk list of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import function for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#import TF function\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Import utility files\n",
    "from utils import read_df, remove_links,remove_comments, clean_sentence, save_object, load_object, make_clustering_objects, weeks_since, stopless_text_generator, months_since, clean_id\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup directories\n",
    "\n",
    "If this is the first time doing this analysis, \n",
    "we first will set up all the directories we need\n",
    "to save and load the models we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directories = ['objects', 'models', 'clusters', 'matricies', 'excels']\n",
    "for dirname in directories:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Model\n",
    "\n",
    "Before begining the rest of our project, we select a name for our model.\n",
    "This name will be used to save and load the files for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"2019_SW_PandC\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Parse and Clean Data\n",
    "\n",
    "We first parse and clean our data. Our data is assumed to be in csv format, \n",
    "in a directory labeled 'data'.\n",
    "Files for all subreddits should be included in the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dataframe from just the posts\n",
    "dfP = read_df('SuicideWatchPosts',extension = \"/*.csv\")\n",
    "\n",
    "#Make comment data frame and join to keep all info\n",
    "dfP[\"type\"] = 'P'\n",
    "dfC = read_df('SuicideWatchComments',extension = \"/*.csv\")\n",
    "dfC[\"type\"] = 'C'\n",
    "df = dfP.merge(dfC, on=['created_utc','id','author','ups','score','downs','subreddit','name', 'type'], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an inspection of our data to ensure nothing went wrong\n",
    "#df.info gives basic info on the DataFrame, specifically make sure column titles contain \n",
    "#all intended fields and the number of entries\n",
    "#The number of entries will tell you how many posts you have\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head() will print out the first 5 entries to the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace all NaN values with the empty string\n",
    "df = df.replace(np.nan, '', regex = True)\n",
    "# Remove any comments that aren't directly to post\n",
    "df = remove_comments(df)\n",
    "#Replace all deleted posts and authors with an empty string\n",
    "df = df.replace(\"\\[deleted\\]\", '', regex = True)\n",
    "\n",
    "#Concatenate title and post text if submission\n",
    "df[\"rawtext\"] = df[\"title\"] + \" \" + df[\"selftext\"] + \" \" +df[\"body\"]\n",
    "\n",
    "#Clean the raw text, removing links, lower casing, removed characters\n",
    "df[\"cleantext\"] = df[\"rawtext\"].apply(remove_links).apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the cleaning was successful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.tail() will print out the last five entries of the df\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "\n",
    "After parsing and cleaning the data we further preprocess the data\n",
    "by removing the common words known as stop words using the help of nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of stop words\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list of stop words for reference\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to the DataFrame for the clean text with stop words removed\n",
    "df[\"stoplesstext\"] = df[\"cleantext\"].apply(lambda s: stopless_text_generator(s,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Posts Lists\n",
    "\n",
    "We now need to create a list of posts.\n",
    "The posts lists will be a list of lists, where each internal list is a list of tokens for each post.\n",
    "We will create a list of posts for each subreddit as well as a total list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataframe again into just posts\n",
    "dfP = df[df[\"type\"] == 'P']\n",
    "\n",
    "#Create list of post text\n",
    "posts = dfP[\"stoplesstext\"].apply(lambda str: str.split()).tolist()\n",
    "\n",
    "#Create list of comment text\n",
    "dfC = df[df[\"type\"] == 'C']\n",
    "comments = dfC[\"stoplesstext\"].apply(lambda str: str.split()).tolist()\n",
    "\n",
    "#Saves scores for later linear regression\n",
    "scores = dfC['score']\n",
    "\n",
    "#Create list of both\n",
    "postsAndComments = posts + comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and match posts and responses\n",
    "The next few steps use the id of the post to get all of its responses together. It cleans the data by removing empty or duplicate comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Get just the cleaned text and post id.\n",
    "dfPtext = dfP[['id', 'cleantext']]\n",
    "#Cleans the comment ID to match it to the post ID\n",
    "dfC[\"id\"] = dfC[\"link_id\"].apply(clean_id)\n",
    "#Clean out duplicates\n",
    "dfC = dfC.drop_duplicates('cleantext').sort_index()\n",
    "#Clean out empty commments\n",
    "dfC = dfC[dfC['cleantext'].map(len) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleantext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1002ce</td>\n",
       "      <td>just wanted to let you know that your chance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002uz</td>\n",
       "      <td>hey  im here if you wanna pm me  if not we c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10033x</td>\n",
       "      <td>it will not be painless   this semester has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1003yf</td>\n",
       "      <td>glad to hear youre regaining control and get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10054c</td>\n",
       "      <td>you should probably warm it back up first  b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                          cleantext\n",
       "0  1002ce    just wanted to let you know that your chance...\n",
       "1  1002uz    hey  im here if you wanna pm me  if not we c...\n",
       "2  10033x    it will not be painless   this semester has ...\n",
       "3  1003yf    glad to hear youre regaining control and get...\n",
       "4  10054c    you should probably warm it back up first  b..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this to get all comments for a post\n",
    "dfCtext = dfC[['id', 'cleantext']]\n",
    "dfCtext = dfCtext.groupby(['id'])['cleantext'].apply(', '.join).reset_index()\n",
    "dfCtext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to get top comment for a post (or top comments if equal score)\n",
    "dfCtoptext = dfC[['id', 'cleantext', 'score']]\n",
    "dfCtoptext = dfCtext[dfCtext['score'] == dfCtext.groupby('id')['score'].transform('max')]\n",
    "dfCtoptext = dfCtext.groupby(['id'])['cleantext'].apply(', '.join).reset_index()\n",
    "dfCtoptext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 56469 entries, 0 to 56468\n",
      "Data columns (total 3 columns):\n",
      "id             56469 non-null object\n",
      "cleantext_x    56469 non-null object\n",
      "cleantext_y    56469 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Merge the posts and their comments by matching their IDs\n",
    "#Change dfCtext to dfCtoptext if only looking at highest scored comments\n",
    "dfMatch = dfPtext.merge(dfCtext, on=['id'], how='inner')\n",
    "dfMatch.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMatch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Analysis\n",
    "\n",
    "After parsing and cleaning the data we run the gensim phraser\n",
    "tool on our text data to join phrases like \"new york city\" \n",
    "together to form the word \"new_york_city\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a phraseDetector to join two word phrases together\n",
    "#This is used to generate a model on all text from both posts and comments\n",
    "two_word_phrases = gensim.models.Phrases(postsAndComments)\n",
    "two_word_phraser = gensim.models.phrases.Phraser(two_word_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want one model for each\n",
    "two_word_phrases_P = gensim.models.Phrases(posts)\n",
    "two_word_phraser_P = gensim.models.phrases.Phraser(two_word_phrases_P)\n",
    "\n",
    "two_word_phrases_C = gensim.models.Phrases(comments)\n",
    "two_word_phraser_C = gensim.models.phrases.Phraser(two_word_phrases_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a phraseDetector to join three word phrases together\n",
    "three_word_phrases = gensim.models.Phrases(two_word_phraser[postsAndComments])\n",
    "three_word_phraser = gensim.models.phrases.Phraser(three_word_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For two models\n",
    "# If you want one model for each\n",
    "three_word_phrases_P = gensim.models.Phrases(two_word_phraser[posts])\n",
    "three_word_phraser_P = gensim.models.phrases.Phraser(three_word_phrases_P)\n",
    "\n",
    "three_word_phrases_C = gensim.models.Phrases(two_word_phraser[comments])\n",
    "three_word_phraser_C = gensim.models.phrases.Phraser(three_word_phrases_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update lists to reflect phrasing\n",
    "postsAndComments = list(three_word_phraser[two_word_phraser[postsAndComments]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = list(three_word_phraser_P[two_word_phraser_P[posts]])\n",
    "comments= list(three_word_phraser_C[two_word_phraser_C[comments]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Update Data frames\n",
    "df[\"phrasetext\"] = df[\"stoplesstext\"].apply(lambda str: \" \".join(three_word_phraser[two_word_phraser[str.split()]]))\n",
    "dfP[\"phrasetext\"] = dfP[\"stoplesstext\"].apply(lambda str: \" \".join(three_word_phraser_P[two_word_phraser_P[str.split()]]))\n",
    "dfC[\"phrasetext\"] = dfC[\"stoplesstext\"].apply(lambda str: \" \".join(three_word_phraser_C[two_word_phraser_C[str.split()]]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>from</th>\n",
       "      <th>from_id</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>type</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>rawtext</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>stoplesstext</th>\n",
       "      <th>phrasetext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give me a reason to not kill myself</td>\n",
       "      <td>1493598118</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>68ivqc</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>/r/SuicideWatch/comments/68ivqc/give_me_a_reas...</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Give me a reason to not kill myself</td>\n",
       "      <td>give me a reason to not kill myself</td>\n",
       "      <td>give reason kill</td>\n",
       "      <td>give reason kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>heartbroken.</td>\n",
       "      <td>1493598528</td>\n",
       "      <td>mareikenebel</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>33</td>\n",
       "      <td>68iwwh</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>/r/SuicideWatch/comments/68iwwh/heartbroken/</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>heartbroken. I'm new to Reddit. my boyfriend u...</td>\n",
       "      <td>heartbroken  im new to reddit  my boyfriend us...</td>\n",
       "      <td>heartbroken im new reddit boyfriend used app l...</td>\n",
       "      <td>heartbroken im new reddit boyfriend used app l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't see any reason to live, but I still ha...</td>\n",
       "      <td>1493599344</td>\n",
       "      <td>Throwyourbluesaway</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>68iz9m</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>/r/SuicideWatch/comments/68iz9m/i_dont_see_any...</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I don't see any reason to live, but I still ha...</td>\n",
       "      <td>i dont see any reason to live  but i still hav...</td>\n",
       "      <td>dont see reason live still havent killed yet p...</td>\n",
       "      <td>dont see reason live still havent_killed_yet p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>All I can think about</td>\n",
       "      <td>1493599645</td>\n",
       "      <td>throwaway10802930</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>68j066</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>/r/SuicideWatch/comments/68j066/all_i_can_thin...</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All I can think about is taking a long drive t...</td>\n",
       "      <td>all i can think about is taking a long drive t...</td>\n",
       "      <td>think taking long drive beach watching waves c...</td>\n",
       "      <td>think taking long drive_beach watching waves c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Idk what to type here</td>\n",
       "      <td>1493599725</td>\n",
       "      <td>helpme11233211</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>17</td>\n",
       "      <td>68j0ed</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>https://www.reddit.com/r/SuicideWatch/comments...</td>\n",
       "      <td>/r/SuicideWatch/comments/68j0ed/idk_what_to_ty...</td>\n",
       "      <td>P</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Idk what to type here I tried to kill myself a...</td>\n",
       "      <td>idk what to type here i tried to kill myself a...</td>\n",
       "      <td>idk type tried kill week ago still feel suicid...</td>\n",
       "      <td>idk type tried kill week_ago still feel suicid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  created_utc  \\\n",
       "0                Give me a reason to not kill myself   1493598118   \n",
       "1                                       heartbroken.   1493598528   \n",
       "2  I don't see any reason to live, but I still ha...   1493599344   \n",
       "3                              All I can think about   1493599645   \n",
       "4                              Idk what to type here   1493599725   \n",
       "\n",
       "               author ups downs num_comments      id name from from_id  ...  \\\n",
       "0                                          6  68ivqc                    ...   \n",
       "1        mareikenebel                     33  68iwwh                    ...   \n",
       "2  Throwyourbluesaway                      2  68iz9m                    ...   \n",
       "3   throwaway10802930                      0  68j066                    ...   \n",
       "4      helpme11233211                     17  68j0ed                    ...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reddit.com/r/SuicideWatch/comments...   \n",
       "1  https://www.reddit.com/r/SuicideWatch/comments...   \n",
       "2  https://www.reddit.com/r/SuicideWatch/comments...   \n",
       "3  https://www.reddit.com/r/SuicideWatch/comments...   \n",
       "4  https://www.reddit.com/r/SuicideWatch/comments...   \n",
       "\n",
       "                                           permalink  type body link_id  \\\n",
       "0  /r/SuicideWatch/comments/68ivqc/give_me_a_reas...     P                \n",
       "1       /r/SuicideWatch/comments/68iwwh/heartbroken/     P                \n",
       "2  /r/SuicideWatch/comments/68iz9m/i_dont_see_any...     P                \n",
       "3  /r/SuicideWatch/comments/68j066/all_i_can_thin...     P                \n",
       "4  /r/SuicideWatch/comments/68j0ed/idk_what_to_ty...     P                \n",
       "\n",
       "  parent_id                                            rawtext  \\\n",
       "0                        Give me a reason to not kill myself     \n",
       "1            heartbroken. I'm new to Reddit. my boyfriend u...   \n",
       "2            I don't see any reason to live, but I still ha...   \n",
       "3            All I can think about is taking a long drive t...   \n",
       "4            Idk what to type here I tried to kill myself a...   \n",
       "\n",
       "                                           cleantext  \\\n",
       "0              give me a reason to not kill myself     \n",
       "1  heartbroken  im new to reddit  my boyfriend us...   \n",
       "2  i dont see any reason to live  but i still hav...   \n",
       "3  all i can think about is taking a long drive t...   \n",
       "4  idk what to type here i tried to kill myself a...   \n",
       "\n",
       "                                        stoplesstext  \\\n",
       "0                                   give reason kill   \n",
       "1  heartbroken im new reddit boyfriend used app l...   \n",
       "2  dont see reason live still havent killed yet p...   \n",
       "3  think taking long drive beach watching waves c...   \n",
       "4  idk type tried kill week ago still feel suicid...   \n",
       "\n",
       "                                          phrasetext  \n",
       "0                                   give reason kill  \n",
       "1  heartbroken im new reddit boyfriend used app l...  \n",
       "2  dont see reason live still havent_killed_yet p...  \n",
       "3  think taking long drive_beach watching waves c...  \n",
       "4  idk type tried kill week_ago still feel suicid...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>from</th>\n",
       "      <th>from_id</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>permalink</th>\n",
       "      <th>type</th>\n",
       "      <th>body</th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>rawtext</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>stoplesstext</th>\n",
       "      <th>phrasetext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1118878</th>\n",
       "      <td></td>\n",
       "      <td>1393629047</td>\n",
       "      <td>megaflubbie</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cfrbi68</td>\n",
       "      <td>t1_cfrbi68</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td>What is wrong? tell your story please. I am in...</td>\n",
       "      <td>t3_1z7x3r</td>\n",
       "      <td>t3_1z7x3r</td>\n",
       "      <td>What is wrong? tell your story please. I am ...</td>\n",
       "      <td>what is wrong  tell your story please  i am ...</td>\n",
       "      <td>wrong tell story please interested</td>\n",
       "      <td>wrong tell story please interested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118879</th>\n",
       "      <td></td>\n",
       "      <td>1393629181</td>\n",
       "      <td>Beetle559</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cfrbk69</td>\n",
       "      <td>t1_cfrbk69</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td>Admins were messaged and the post has been rem...</td>\n",
       "      <td>t3_1z7spg</td>\n",
       "      <td>t3_1z7spg</td>\n",
       "      <td>Admins were messaged and the post has been r...</td>\n",
       "      <td>admins were messaged and the post has been r...</td>\n",
       "      <td>admins messaged post removed ill deleting thanks</td>\n",
       "      <td>admins messaged post removed ill deleting thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118880</th>\n",
       "      <td></td>\n",
       "      <td>1393629192</td>\n",
       "      <td>JamjamR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cfrbkc7</td>\n",
       "      <td>t1_cfrbkc7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td>Keep your head up! If possible, try to turn of...</td>\n",
       "      <td>t3_1z7so3</td>\n",
       "      <td>t3_1z7so3</td>\n",
       "      <td>Keep your head up! If possible, try to turn ...</td>\n",
       "      <td>keep your head up  if possible  try to turn ...</td>\n",
       "      <td>keep head possible try turn phone computer go ...</td>\n",
       "      <td>keep head possible try turn phone computer go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118882</th>\n",
       "      <td></td>\n",
       "      <td>1393629209</td>\n",
       "      <td>skyqween</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cfrbkk7</td>\n",
       "      <td>t1_cfrbkk7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td>Please read the sidebar rules. Advertising you...</td>\n",
       "      <td>t3_1z7giv</td>\n",
       "      <td>t3_1z7giv</td>\n",
       "      <td>Please read the sidebar rules. Advertising y...</td>\n",
       "      <td>please read the sidebar rules  advertising y...</td>\n",
       "      <td>please read sidebar rules advertising helper m...</td>\n",
       "      <td>please_read_sidebar rules_advertising helper_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118888</th>\n",
       "      <td></td>\n",
       "      <td>1393629936</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>cfrbv31</td>\n",
       "      <td>t1_cfrbv31</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td>t3_1z7x3r</td>\n",
       "      <td>t3_1z7x3r</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title  created_utc       author ups downs num_comments       id  \\\n",
       "1118878         1393629047  megaflubbie   1     0               cfrbi68   \n",
       "1118879         1393629181    Beetle559   1     0               cfrbk69   \n",
       "1118880         1393629192      JamjamR   1     0               cfrbkc7   \n",
       "1118882         1393629209     skyqween   1     0               cfrbkk7   \n",
       "1118888         1393629936                1     0               cfrbv31   \n",
       "\n",
       "               name from from_id  ... url permalink  type  \\\n",
       "1118878  t1_cfrbi68               ...                   C   \n",
       "1118879  t1_cfrbk69               ...                   C   \n",
       "1118880  t1_cfrbkc7               ...                   C   \n",
       "1118882  t1_cfrbkk7               ...                   C   \n",
       "1118888  t1_cfrbv31               ...                   C   \n",
       "\n",
       "                                                      body    link_id  \\\n",
       "1118878  What is wrong? tell your story please. I am in...  t3_1z7x3r   \n",
       "1118879  Admins were messaged and the post has been rem...  t3_1z7spg   \n",
       "1118880  Keep your head up! If possible, try to turn of...  t3_1z7so3   \n",
       "1118882  Please read the sidebar rules. Advertising you...  t3_1z7giv   \n",
       "1118888                                                     t3_1z7x3r   \n",
       "\n",
       "         parent_id                                            rawtext  \\\n",
       "1118878  t3_1z7x3r    What is wrong? tell your story please. I am ...   \n",
       "1118879  t3_1z7spg    Admins were messaged and the post has been r...   \n",
       "1118880  t3_1z7so3    Keep your head up! If possible, try to turn ...   \n",
       "1118882  t3_1z7giv    Please read the sidebar rules. Advertising y...   \n",
       "1118888  t3_1z7x3r                                                      \n",
       "\n",
       "                                                 cleantext  \\\n",
       "1118878    what is wrong  tell your story please  i am ...   \n",
       "1118879    admins were messaged and the post has been r...   \n",
       "1118880    keep your head up  if possible  try to turn ...   \n",
       "1118882    please read the sidebar rules  advertising y...   \n",
       "1118888                                                      \n",
       "\n",
       "                                              stoplesstext  \\\n",
       "1118878                 wrong tell story please interested   \n",
       "1118879   admins messaged post removed ill deleting thanks   \n",
       "1118880  keep head possible try turn phone computer go ...   \n",
       "1118882  please read sidebar rules advertising helper m...   \n",
       "1118888                                                      \n",
       "\n",
       "                                                phrasetext  \n",
       "1118878                 wrong tell story please interested  \n",
       "1118879   admins messaged post removed ill deleting thanks  \n",
       "1118880  keep head possible try turn phone computer go ...  \n",
       "1118882  please_read_sidebar rules_advertising helper_m...  \n",
       "1118888                                                     \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the dataframe was updated correctly\n",
    "#Can take a long time, prints progress\n",
    "#for i in range(len(allPosts)):\n",
    "#   if i % 500 == 0:\n",
    "#        print(i)\n",
    "#    if not \" \".join(allPosts[i]) == list(df[\"phrasetext\"])[i]:\n",
    "#       print(\"index :\" + str(i) + \" is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Element\n",
    "\n",
    "Add a column to the DataFrame that indicates which week the post occured in\n",
    "Add a column to the DataFrame that indicates which month the post occured in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseyear = int(input('What year does the data start in: '))\n",
    "df[\"week_no\"] = df[\"created_utc\"].apply(lambda utc: int(weeks_since(utc, baseyear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month_no\"] = df[\"created_utc\"].apply(lambda utc: int(months_since(utc, baseyear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Saving\n",
    "\n",
    "After cleaning and parsing all of our data, we can now\n",
    "save it, so that we can analysis it later without having\n",
    "to go through lengthy computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Word2Vec Model\n",
    "\n",
    "After all of our data has been parsed and saved, \n",
    "we generate our Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the minimum word count to 10. This removes all words that appear less than 10 times in the data\n",
    "minimum_word_count = 10\n",
    "# Set skip gram to 1. This sets gensim to use the skip gram model instead of the Continuous Bag of Words model\n",
    "skip_gram = 1\n",
    "# Set Hidden layer size to 300.\n",
    "hidden_layer_size = 300\n",
    "# Set the window size to 5. \n",
    "window_size = 5\n",
    "# Set hierarchical softmax to 1. This sets gensim to use hierarchical softmax\n",
    "hierarchical_softmax = 1\n",
    "# Set negative sampling to 20. This is good for relatively small data sets, but becomes harder for larger datasets\n",
    "negative_sampling = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d8a3ad70e2cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build the model on all the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model = gensim.models.Word2Vec(postsAndComments, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n\u001b[0;32m----> 3\u001b[0;31m                                    window = window_size, hs = hierarchical_softmax, negative = negative_sampling)\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the model on all the text\n",
    "model = gensim.models.Word2Vec(postsAndComments, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n",
    "                                   window = window_size, hs = hierarchical_softmax, negative = negative_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-12d4a69bb684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#If building two models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m modelP = gensim.models.Word2Vec(posts, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n\u001b[0;32m----> 3\u001b[0;31m                                    window = window_size, hs = hierarchical_softmax, negative = negative_sampling)\n\u001b[0m\u001b[1;32m      4\u001b[0m modelC = gensim.models.Word2Vec(comments, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n\u001b[1;32m      5\u001b[0m                                    window = window_size, hs = hierarchical_softmax, negative = negative_sampling)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#If building two models\n",
    "modelP = gensim.models.Word2Vec(posts, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n",
    "                                   window = window_size, hs = hierarchical_softmax, negative = negative_sampling)\n",
    "modelC = gensim.models.Word2Vec(comments, min_count = minimum_word_count, sg = skip_gram, size = hidden_layer_size,\n",
    "                                   window = window_size, hs = hierarchical_softmax, negative = negative_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model test\n",
    "\n",
    "After generating our model, we run some basic tests\n",
    "to ensure that it has captured some semantic information results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('models/' + model_name + '.model')\n",
    "modelP = gensim.models.Word2Vec.load('models/' + '2019_SW_P' + '.model')\n",
    "modelC = gensim.models.Word2Vec.load('models/' + '2019_SW_C' + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ive_seen_alcoholics', 0.5500342845916748),\n",
       " ('fault_responsibility_ensure', 0.5276337265968323),\n",
       " ('clean_sober', 0.505244255065918),\n",
       " ('battling_addiction_well', 0.5051607489585876),\n",
       " ('happy_fulfilling_lives', 0.4990171790122986),\n",
       " ('drunk', 0.47476282715797424),\n",
       " ('sobriety', 0.45957720279693604),\n",
       " ('worse_benzos_special', 0.4538528323173523),\n",
       " ('quit_drinking_start_addressing', 0.44979527592658997),\n",
       " ('stay_sober', 0.44713395833969116)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = [\"sober\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drunk', 0.5707206130027771),\n",
       " ('clean_sober', 0.5328336954116821),\n",
       " ('drinking', 0.5269137620925903),\n",
       " ('relapsed', 0.5151023268699646),\n",
       " ('stayed_sober', 0.5045028924942017),\n",
       " ('stay_sober', 0.5033601522445679),\n",
       " ('smoking_cigs', 0.49789664149284363),\n",
       " ('intoxicated', 0.49661320447921753),\n",
       " ('rehab', 0.4857616722583771),\n",
       " ('detoxed', 0.4766559898853302)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelP.wv.most_similar(positive = [\"sober\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happy_fulfilling_lives', 0.5655555129051208),\n",
       " ('ive_seen_alcoholics', 0.5632500648498535),\n",
       " ('quit_drinking_start_addressing', 0.4905615448951721),\n",
       " ('waving', 0.42963454127311707),\n",
       " ('well_rested', 0.4177454710006714),\n",
       " ('aa_na', 0.4107852578163147),\n",
       " ('drug_addicted', 0.3982229232788086),\n",
       " ('altered_state', 0.39123648405075073),\n",
       " ('recovering', 0.38190215826034546),\n",
       " ('relapsed', 0.3588559627532959)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelC.wv.most_similar(positive = [\"sober\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('husband', 0.4344366788864136),\n",
       " ('mother', 0.42998647689819336),\n",
       " ('okay_mourn_passing', 0.4196780323982239),\n",
       " ('sister', 0.41342467069625854),\n",
       " ('forced_abortion', 0.4105724096298218),\n",
       " ('still_connected_perfectly', 0.4092947244644165),\n",
       " ('previous_marriage', 0.40773332118988037),\n",
       " ('remarried', 0.39508742094039917),\n",
       " ('siblings', 0.38945552706718445),\n",
       " ('fiance', 0.3832973837852478)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = [\"father\", \"woman\"], negative = [\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parents', 0.3977805972099304),\n",
       " ('mom', 0.3689156174659729),\n",
       " ('sad_hear_incompetence', 0.3616834282875061),\n",
       " ('irate', 0.3553171753883362),\n",
       " ('upset', 0.3504619598388672),\n",
       " ('unsupportive', 0.34635305404663086),\n",
       " ('family_members', 0.3441495895385742),\n",
       " ('vital_essential_personnel_supportive', 0.34184131026268005),\n",
       " ('angry_frustrated', 0.3396082818508148),\n",
       " ('everything_short_sweet', 0.339534193277359)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive = [\"family\", \"angry\"], negative = [\"love\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "After generating our model, and runing some basic tests,\n",
    "we now save it so that we can analysis it later without having\n",
    "to go through lengthy computations. We also delete and then reload\n",
    "the model, as an example of how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save('models/' + model_name + '.model')\n",
    "modelP.save('models/' + model_name + '-posts' + '.model')\n",
    "modelC.save('models/' + model_name + '-comments' + '.model')\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load('models/' + model_name + '.model')\n",
    "modelP = gensim.models.Word2Vec.load('models/' + model_name + '-posts' + '.model')\n",
    "modelC = gensim.models.Word2Vec.load('models/' + model_name + '-comments' + '.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Matricies\n",
    "\n",
    "After generating our Word2Vec Model, we generate \n",
    "a collection of matricies that will be useful for\n",
    "analysis. This includes a Words By feature matrix,\n",
    "and a Post By Words Matrix. Note, we will use camelCase \n",
    "for matrix names, and only matrix names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of words used\n",
    "vocab_list = sorted(list(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_listP = sorted(list(modelP.wv.vocab))\n",
    "vocab_listC = sorted(list(modelC.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the word vectors\n",
    "vecs = []\n",
    "for word in vocab_list:\n",
    "    vecs.append(model.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecsP = []\n",
    "for word in vocab_listP:\n",
    "    vecsP.append(modelP.wv[word].tolist())\n",
    "    \n",
    "vecsC = []\n",
    "for word in vocab_listC:\n",
    "    vecsC.append(modelC.wv[word].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_listP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68601, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change array format into numpy array\n",
    "WordsByFeatures = np.array(vecs)\n",
    "WordsByFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44155, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just words from posts\n",
    "PostWordsByFeatures = np.array(vecsP)\n",
    "PostWordsByFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45621, 300)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just words from comments\n",
    "CommentWordsByFeatures = np.array(vecsC)\n",
    "CommentWordsByFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this as check to see they match\n",
    "vocab_list[58355]\n",
    "print(WordsByFeatures[58355])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(376604, 68601)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvec = CountVectorizer(vocabulary = vocab_list, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "\n",
    "# Make Posts By Words Matrix\n",
    "PostsByWords = countvec.fit_transform(posts)\n",
    "PostsByWords.shape\n",
    "\n",
    "#Make comments by words matrix\n",
    "CommentsByWords = countvec.fit_transform(comments)\n",
    "CommentsByWords.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-ebf095536a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make Posts By Words Matrix- only post words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mPostsByPostWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountvecP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mPostsByPostWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "countvecP = CountVectorizer(vocabulary = vocab_listP, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "\n",
    "# Make Posts By Words Matrix- only post words\n",
    "PostsByPostWords = countvecP.fit_transform(posts)\n",
    "PostsByPostWords.shape\n",
    "\n",
    "countvecC = CountVectorizer(vocabulary = vocab_listC, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "\n",
    "#Make comments by words matrix- only comment words\n",
    "CommentsByCommentWords = countvecC.fit_transform(comments)\n",
    "CommentsByCommentWords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56469, 45621)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countvecP = CountVectorizer(vocabulary = vocab_listP, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "countvecC = CountVectorizer(vocabulary = vocab_listC, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "\n",
    "#Make post mapped to comments matrices\n",
    "postsMatched = dfMatch[\"cleantext_x\"].apply(lambda str: str.split()).tolist()\n",
    "commentsMatched = dfMatch[\"cleantext_y\"].apply(lambda str: str.split()).tolist()\n",
    "\n",
    "#This matrix will be used for association rules, so these will use the two separate models\n",
    "PostsMatchedByWords = countvecP.fit_transform(postsMatched)\n",
    "PostsMatchedByWords.shape\n",
    "\n",
    "CommentsMatchedByWords = countvecC.fit_transform(commentsMatched)\n",
    "CommentsMatchedByWords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make for both\n",
    "PostsAndCommentsByWords = countvec.fit_transform(postsAndComments)\n",
    "PostsAndCommentsByWords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check that both shapes are equal to lists and each other\n",
    "print(PostsMatchedByWords.shape[0] == len(postsMatched) == CommentsMatchedByWords.shape[0] == len(commentsMatched))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Matrix tests\n",
    "\n",
    "After generating our matricies, we run some basic tests\n",
    "to ensure that they seem resaonable later without having\n",
    "to go through lengthy computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check that PostsByWords is the number of Posts by the number of words\n",
    "print(PostsByWords.shape[0] == len(posts))\n",
    "#Same for comments\n",
    "print(CommentsByWords.shape[0] == len(comments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Matricies\n",
    "\n",
    "After generating our matricies, we save them so we can \n",
    "analyze them later without having to go through lengthy\n",
    "computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PostsByWords,'matricies/', model_name + \"-PostsByWords\")\n",
    "save_object(CommentsByWords,'matricies/', model_name + \"-CommentsByWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(CommentsByWords,'matricies/', model_name + \"-CommentsByWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PostsMatchedByWords,'matricies/', model_name + \"-PostsMatchedByWords\")\n",
    "save_object(CommentsMatchedByWords,'matricies/', model_name + \"-CommentsMatchedByWords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Word Clusters\n",
    "\n",
    "Now that we have generated and saved our matricies,\n",
    "we will proceed to generate word clusters using \n",
    "kmeans clustering, and save them for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fast way if u know num clusters you want\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 100\n",
    "fit = []\n",
    "kmeans = KMeans(n_clusters = num_clusters, random_state = 42).fit(WordsByFeatures)\n",
    "save_object(kmeans, 'clusters/', model_name + \"-words-cluster_model-\" + str(num_clusters))\n",
    "fit.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# can take a long time\n",
    "# get the fit for different values of K\n",
    "test_points = [12] + list(range(25, 101, 25))\n",
    "fit = []\n",
    "for point in test_points:\n",
    "    print(point)\n",
    "    kmeans = KMeans(n_clusters = point, random_state = 42).fit(WordsByFeatures)\n",
    "    save_object(kmeans, 'clusters/', model_name + \"-words-cluster_model-\" + str(point))\n",
    "    fit.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Create clusters for just posts\n",
    "fitP = []\n",
    "for point in test_points:\n",
    "    print(point)\n",
    "    kmeansP = KMeans(n_clusters = point, random_state = 42).fit(PostWordsByFeatures)\n",
    "    save_object(kmeans, 'clusters/', model_name + \"-post-words-cluster_model-\" + str(point))\n",
    "    fit.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Create clusters for just posts\n",
    "fitC = []\n",
    "for point in test_points:\n",
    "    print(point)\n",
    "    kmeansC = KMeans(n_clusters = point, random_state = 42).fit(CommentWordsByFeatures)\n",
    "    save_object(kmeans, 'clusters/', model_name + \"-comment-words-cluster_model-\" + str(point))\n",
    "    fit.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(fit, 'objects/', model_name + \"-words\" + \"-fit\")\n",
    "save_object(fitP, 'objects/', model_name + \"-post-words\" + \"-fit\")\n",
    "save_object(fitC, 'objects/', model_name + \"-comment-words\" + \"-fit\")\n",
    "save_object(test_points, 'objects/', model_name + \"-words\" + \"-test_points\")\n",
    "del fit\n",
    "del test_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Months By Posts Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to add time portion for comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did not do time element on responses, but it would be easy to implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create proper shape matrix with all 0s\n",
    "MonthsByAllPosts = np.zeros( (df[\"month_no\"].max()+1, len(df) ) )\n",
    "for s in subreddits:\n",
    "    subreddit_info[s]['MonthsByPosts'] = np.zeros( ( subreddit_info[s]['df']['month_no'].max()+1 , len(subreddit_info[s]['df'] ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MonthsByAllPosts.shape)\n",
    "for s in subreddits:\n",
    "    print(s)\n",
    "    print(subreddit_info[s][\"MonthsByPosts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for post in df.itertuples():\n",
    "    MonthsByAllPosts[post[-1]][i] = 1\n",
    "    i += 1\n",
    "print(MonthsByAllPosts.shape)\n",
    "\n",
    "for i in range(MonthsByAllPosts.shape[1]):\n",
    "    assert MonthsByAllPosts[:,i].sum() == 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    i = 0\n",
    "    for post in subreddit_info[s]['df'].itertuples():\n",
    "        subreddit_info[s]['MonthsByPosts'][post[-1]][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(subreddit_info[s]['MonthsByPosts'].shape)\n",
    "    for i in range(subreddit_info[s]['MonthsByPosts'].shape[1]):\n",
    "        assert subreddit_info[s]['MonthsByPosts'][:,i].sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(MonthsByAllPosts, 'matricies/', model_name + \"-MonthsByAllPosts\")\n",
    "\n",
    "for s in subreddits:\n",
    "    save_object(subreddit_info[s]['MonthsByPosts'], 'matricies/', model_name + \"-\" + subreddit_info[s]['abbr'] + \"MonthsByPosts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Posts By Clusters Matrix \n",
    "\n",
    "Now we need to create the PostsByClusters matrix using the below equation:\n",
    "\n",
    "PostsByWords X WordsByClusters = PostsByClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 68601)\n",
      "(68601, 100)\n"
     ]
    }
   ],
   "source": [
    "#Initialize a word clustering to use for all posts and comments\n",
    "num_word_clusters = 100\n",
    "kmeans = load_object('clusters/', model_name + '-words-cluster_model-' + str(num_word_clusters))\n",
    "\n",
    "clusters = make_clustering_objects(model, kmeans, vocab_list, WordsByFeatures)\n",
    "\n",
    "clusterWords = list(map(lambda x: list(map(lambda y: y[0] , x[\"word_list\"])), clusters))\n",
    "\n",
    "countvec = CountVectorizer(vocabulary = vocab_list, analyzer = (lambda lst:list(map((lambda s: s), lst))), min_df = 0)\n",
    "\n",
    "#Create ClustersByWords matrix\n",
    "ClusterByWords = countvec.fit_transform(clusterWords)\n",
    "\n",
    "#Look at Dimensions\n",
    "print(ClusterByWords.shape)\n",
    "\n",
    "#Create WordsByClusters\n",
    "WordsByClusters = ClusterByWords.transpose()\n",
    "print(WordsByClusters.shape)\n",
    "save_object(WordsByClusters, 'matricies/', model_name + \"WordsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "\n",
    "#Put into readable excel doc\n",
    "wordsByC_df = pd.DataFrame(ClusterByWords)\n",
    "filepath = 'excels/' + model_name + \"-ClusterByWords-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "wordsByC_df.to_excel(filepath, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 44155)\n",
      "(44155, 100)\n"
     ]
    }
   ],
   "source": [
    "#Create post words by clusters matrix\n",
    "kmeansP = load_object('clusters/', model_name + '-post-words-cluster_model-' + str(num_word_clusters))\n",
    "\n",
    "clustersP = make_clustering_objects(modelP, kmeansP, vocab_listP, PostWordsByFeatures)\n",
    "\n",
    "PclusterWords = list(map(lambda x: list(map(lambda y: y[0] , x[\"word_list\"])), clustersP))\n",
    "\n",
    "countvecP = CountVectorizer(vocabulary = vocab_listP, analyzer = (lambda lst:list(map((lambda s: s), lst))), min_df = 0)\n",
    "\n",
    "#Create ClustersByWords matrix\n",
    "ClusterByPostWords = countvecP.fit_transform(PclusterWords)\n",
    "\n",
    "#Look at Dimensions\n",
    "print(ClusterByPostWords.shape)\n",
    "\n",
    "#Create WordsByClusters\n",
    "PostWordsByClusters = ClusterByPostWords.transpose()\n",
    "print(PostWordsByClusters.shape)\n",
    "save_object(PostWordsByClusters, 'matricies/', model_name + \"PostWordsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "\n",
    "#Put into readable excel doc\n",
    "wordsByC_df = pd.DataFrame(ClusterByPostWords)\n",
    "filepath = 'excels/' + model_name + \"-ClusterByPostWords-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "wordsByC_df.to_excel(filepath, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 45621)\n",
      "(45621, 100)\n"
     ]
    }
   ],
   "source": [
    "#Create clusters for just comments\n",
    "kmeansC = load_object('clusters/', model_name + '-comment-words-cluster_model-' + str(num_word_clusters))\n",
    "\n",
    "clustersC = make_clustering_objects(modelC, kmeansC, vocab_listC, CommentWordsByFeatures)\n",
    "\n",
    "CclusterWords = list(map(lambda x: list(map(lambda y: y[0] , x[\"word_list\"])), clustersC))\n",
    "\n",
    "countvecC = CountVectorizer(vocabulary = vocab_listC, analyzer = (lambda lst:list(map((lambda s: s), lst))), min_df = 0)\n",
    "\n",
    "#Create ClustersByWords matrix\n",
    "ClusterByCommentWords = countvecC.fit_transform(CclusterWords)\n",
    "\n",
    "#Look at Dimensions\n",
    "print(ClusterByCommentWords.shape)\n",
    "\n",
    "#Create WordsByClusters\n",
    "CommentWordsByClusters = ClusterByCommentWords.transpose()\n",
    "print(CommentWordsByClusters.shape)\n",
    "save_object(CommentWordsByClusters, 'matricies/', model_name + \"CommentWordsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "\n",
    "#Put into readable excel doc\n",
    "wordsByC_df = pd.DataFrame(ClusterByCommentWords)\n",
    "filepath = 'excels/' + model_name + \"-ClusterByCommentWords-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "wordsByC_df.to_excel(filepath, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for word in clusters[i]['word_list']:\n",
    "        if word[0] == 'williams':\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(228962, 100)\n",
      "(376604, 100)\n"
     ]
    }
   ],
   "source": [
    "# Create Posts By Clusters and comments by clusters through matrix multiplication\n",
    "PostsByClusters = PostsByWords.dot(WordsByClusters.toarray())\n",
    "CommentsByClusters = CommentsByWords.dot(WordsByClusters.toarray())\n",
    "\n",
    "PostsByClusters = np.matrix(PostsByClusters)\n",
    "CommentsByClusters = np.matrix(CommentsByClusters)\n",
    "\n",
    "print(PostsByClusters.shape)\n",
    "print(CommentsByClusters.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostsMatchedByClusters = PostsMatchedByWords.dot(PostWordsByClusters.toarray())\n",
    "CommentsMatchedByClusters = CommentsMatchedByWords.dot(CommentWordsByClusters.toarray())\n",
    "\n",
    "PostsMatchedByClusters = np.matrix(PostsMatchedByClusters)\n",
    "CommentsMatchedByClusters = np.matrix(CommentsMatchedByClusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCmatchedByClusters = np.hstack((PostsMatchedByClusters, CommentsMatchedByClusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PostsByClusters, 'matricies/', model_name + \"-PostsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "save_object(CommentsByClusters, 'matricies/', model_name + \"-CommentsByClusters-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PostsAndCommentsByClusters,'matricies/', model_name + \"-PostsAndCommentsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "\n",
    "for s in subreddits:\n",
    "    save_object(subreddit_info[s][\"PostsByClusters\"], 'matricies/', model_name + '-' + subreddit_info[s][\"abbr\"] + \"PostsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "    save_object(subreddit_info[s][\"CommentsByClusters\"], 'matricies/', model_name + '-' + subreddit_info[s][\"abbr\"] + \"CommentsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "    save_object(subreddit_info[s][\"PostsAndCommentsByClusters\"], 'matricies/', model_name + '-' + subreddit_info[s][\"abbr\"] + \"PostsAndCommentsByClusters-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PostsMatchedByClusters, 'matricies/', model_name + \"-PostMatchedsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "save_object(CommentsMatchedByClusters, 'matricies/', model_name + \"-CommentsMatchedByClusters-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(PCmatchedByClusters, 'matricies/', model_name + \"-PCMatchedByClusters-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression of Comment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "CommentsByClusters = load_object('matricies/', model_name + \"-CommentsByClusters-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make comment by score matrix\n",
    "CommentsByScore = np.matrix(scores).T\n",
    "CommentsByScore.shape\n",
    "save_object(CommentsByScore, 'matricies/', model_name + \"-CommentsByScore-\" + str(num_word_clusters) + 'clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "We did not use this part or the later trends over time part for the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add column on just 1s to CommentByClusters for B0 value\n",
    "n,m = CommentsByClusters.shape \n",
    "X0 = np.ones((n,1))\n",
    "CommentsByClustersOnes = np.hstack((X0, CommentsByClusters))\n",
    "CommentsByClustersOnes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvec = TfidfVectorizer(vocabulary = vocab_list, analyzer = (lambda lst:list(map((lambda s:s), lst))), min_df = 0)\n",
    "\n",
    "# Make Posts By Words Matrix\n",
    "TFIDFPostsByWords = tfidfvec.fit_transform(posts)\n",
    "TFIDFPostsByWords.shape\n",
    "TFIDFCommentsByWords = tfidfvec.fit_transform(comments)\n",
    "TFIDFCommentsByWords.shape\n",
    "TFIDFPostsAndCommentsByWords = tfidfvec.fit_transform(postsAndComments)\n",
    "TFIDFPostsAndCommentsByWords.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create TFIDF Posts By Clusters through matrix multiplication\n",
    "TFIDFPostsByClusters = TFIDFPostsByWords.dot(WordsByClusters.toarray())\n",
    "TFIDFPostsByClusters = np.matrix(TFIDFPostsByClusters)\n",
    "TFIDFCommentsByClusters = TFIDFCommentsByWords.dot(WordsByClusters.toarray())\n",
    "TFIDFCommentsByClusters = np.matrix(TFIDFCommentsByClusters)\n",
    "TFIDFPostsAndCommentsByClusters = TFIDFPostsAndCommentsByWords.dot(WordsByClusters.toarray())\n",
    "TFIDFPostsAndCommentsByClusters = np.matrix(TFIDFPostsAndCommentsByClusters)\n",
    "\n",
    "print(TFIDFPostsByClusters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TFIDFPostsByClusters[:,36]\n",
    "x = np.asarray(x.transpose())\n",
    "x = x[0]\n",
    "idx = np.argsort(x)\n",
    "idx[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in idx[-10:]:\n",
    "    print(i)\n",
    "    print(df.iloc[i]['rawtext'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted( range(len(x)), key = lambda i: x[i], reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Months By Clusters Matrix \n",
    "\n",
    "Now we need to create the MonthsByClusters matrix using the below equation:\n",
    "\n",
    "MonthsByPosts X PostsByClusters = MonthsByClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MonthsByClusters matrix through matrix multiplication\n",
    "ALL_MonthsByClusters = MonthsByAllPosts.dot(PostsByClusters)\n",
    "ALL_MonthsByClusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    subreddit_info[s][\"MonthsByClusters\"] = subreddit_info[s][\"MonthsByPosts\"].dot(subreddit_info[s][\"PostsByClusters\"])\n",
    "    print(s)\n",
    "    print(subreddit_info[s][\"MonthsByClusters\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(ALL_MonthsByClusters, 'matricies/', model_name + \"-ALL_MonthsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "for s in subreddits:\n",
    "    save_object(subreddit_info[s][\"MonthsByClusters\"], 'matricies/', model_name + \"-\" + subreddit_info[s][\"abbr\"] + \"_MonthsByClusters-\" + str(num_word_clusters) + 'clusters')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Months By Clusters Matrix Normalizations\n",
    "\n",
    "Normalize the Months By Clusters matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amXc_df = pd.DataFrame(ALL_MonthsByClusters)\n",
    "filepath = 'excels/' + model_name + \"-ALL_MonthsByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "amXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_L2A1MonthsByClusters = normalize(ALL_MonthsByClusters, norm='l2', axis = 1)\n",
    "save_object(ALL_L2A1MonthsByClusters, 'matricies/', model_name + \"-ALL_L2A1MonthsByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "L2A1amXc_df = pd.DataFrame(ALL_L2A1MonthsByClusters)\n",
    "filepath = 'excels/' + model_name + \"-ALL_L2A1MonthsByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "L2A1amXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    mXc_df = pd.DataFrame(subreddit_info[s][\"MonthsByClusters\"])\n",
    "    filepath = 'excels/' + model_name + '-' + subreddit_info[s]['abbr'] + \"-MonthsByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "    mXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    L2A1Normalized = normalize(subreddit_info[s][\"MonthsByClusters\"], norm = 'l2', axis = 1)\n",
    "    save_object(L2A1Normalized, 'matricies/', model_name+'-'+subreddit_info[s]['abbr']+'-L2A1MonthsByClusters-'+str(num_word_clusters)+'clusters' )\n",
    "    NmXc_df = pd.DataFrame(L2A1Normalized)\n",
    "    filepath = 'excels/'+model_name+'-'+subreddit_info[s]['abbr']+'-L2A1MonthsByClusters'+str(num_word_clusters)+'clusters.xlsx'\n",
    "    NmXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Weeks By Posts Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create proper shape matrix with all 0s\n",
    "WeeksByAllPosts = np.zeros( (df[\"week_no\"].max()+1, len(df) ) )\n",
    "for s in subreddits:\n",
    "    subreddit_info[s]['WeeksByPosts'] = np.zeros( ( subreddit_info[s]['df']['week_no'].max()+1 , len(subreddit_info[s]['df'] ) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WeeksByAllPosts.shape)\n",
    "for s in subreddits:\n",
    "    print(s)\n",
    "    print(subreddit_info[s][\"WeeksByPosts\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for post in df.itertuples():\n",
    "    WeeksByAllPosts[post[-2]][i] = 1\n",
    "    i += 1\n",
    "\n",
    "print(WeeksByAllPosts.shape)\n",
    "for i in range(WeeksByAllPosts.shape[1]):\n",
    "    assert WeeksByAllPosts[:,i].sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    i = 0\n",
    "    for post in subreddit_info[s]['df'].itertuples():\n",
    "        subreddit_info[s]['WeeksByPosts'][post[-2]][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(subreddit_info[s]['WeeksByPosts'].shape)\n",
    "    for i in range(subreddit_info[s]['WeeksByPosts'].shape[1]):\n",
    "        assert subreddit_info[s]['WeeksByPosts'][:,i].sum() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(WeeksByAllPosts, 'matricies/', model_name + \"-WeeksByAllPosts\")\n",
    "\n",
    "for s in subreddits:\n",
    "    save_object(subreddit_info[s]['WeeksByPosts'], 'matricies/', model_name + \"-\" + subreddit_info[s]['abbr'] + \"WeeksByPosts\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Weeks By Clusters Matrix \n",
    "\n",
    "Now we need to create the WeeksByClusters matrix using the below equation:\n",
    "\n",
    "WeeksByPosts X PostsByClusters = WeeksByClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MonthsByClusters matrix through matrix multiplication\n",
    "ALL_WeeksByClusters = WeeksByAllPosts.dot(allPostsByClusters)\n",
    "ALL_WeeksByClusters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    subreddit_info[s][\"WeeksByClusters\"] = subreddit_info[s][\"WeeksByPosts\"].dot(subreddit_info[s][\"PostsByClusters\"])\n",
    "    print(s)\n",
    "    print(subreddit_info[s][\"WeeksByClusters\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(ALL_WeeksByClusters, 'matricies/', model_name + \"-ALL_WeeksByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "for s in subreddits:\n",
    "    save_object(subreddit_info[s][\"WeeksByClusters\"], 'matricies/', model_name + \"-\" + subreddit_info[s][\"abbr\"] + \"_WeeksByClusters-\" + str(num_word_clusters) + 'clusters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Weeks By Clusters Matrix Normalizations\n",
    "\n",
    "Normalize the Weeks By Clusters matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awXc_df = pd.DataFrame(ALL_WeeksByClusters)\n",
    "filepath = 'excels/' + model_name + \"-ALL_WeeksByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "awXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_L2A1WeeksByClusters = normalize(ALL_WeeksByClusters, norm='l2', axis = 1)\n",
    "save_object(ALL_L2A1WeeksByClusters, 'matricies/', model_name + \"ALL_L2A1WeeksByClusters-\" + str(num_word_clusters) + 'clusters')\n",
    "L2A1awXc_df = pd.DataFrame(ALL_L2A1WeeksByClusters)\n",
    "filepath = 'excels/' + model_name + \"-ALL_L2A1WeeksByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "L2A1awXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    wXc_df = pd.DataFrame(subreddit_info[s][\"WeeksByClusters\"])\n",
    "    filepath = 'excels/' + model_name + '-' + subreddit_info[s]['abbr'] + \"-WeeksByClusters-\" + str(num_word_clusters) + 'clusters.xlsx'\n",
    "    wXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in subreddits:\n",
    "    L2A1Normalized = normalize(subreddit_info[s][\"WeeksByClusters\"], norm = 'l2', axis = 1)\n",
    "    save_object(L2A1Normalized, 'matricies/', model_name+'-'+subreddit_info[s]['abbr']+'-L2A1WeeksByClusters-'+str(num_word_clusters)+'clusters' )\n",
    "    NwXc_df = pd.DataFrame(L2A1Normalized)\n",
    "    filepath = 'excels/'+model_name+'-'+subreddit_info[s]['abbr']+'-L2A1WeeksByClusters'+str(num_word_clusters)+'clusters.xlsx'\n",
    "    NwXc_df.to_excel(filepath, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dep_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(an_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dep_df) + len(sw_df) + len(an_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depPostsByWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dep = 0\n",
    "not_in_dep = 0\n",
    "for i in range(depPostsByWords.shape[1]):\n",
    "    if i % 1000 == 0:\n",
    "            print((i / 90335) * 100)\n",
    "            print(\"in_dep: \", in_dep)\n",
    "            print(\"not_in_dep: \", not_in_dep)\n",
    "    if depPostsByWords[:,i].sum() < 10:\n",
    "        not_in_dep += 1\n",
    "    else:\n",
    "        in_dep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_dep)\n",
    "print(not_in_dep)\n",
    "print(in_dep + not_in_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sw = 0\n",
    "not_in_sw = 0\n",
    "for i in range(swPostsByWords.shape[1]):\n",
    "    if i % 1000 == 0:\n",
    "            print((i / 90335) * 100)\n",
    "            print(\"in_sw: \", in_sw)\n",
    "            print(\"not_in_sw: \", not_in_sw)\n",
    "    if swPostsByWords[:,i].sum() < 10:\n",
    "        not_in_sw += 1\n",
    "    else:\n",
    "        in_sw += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_sw)\n",
    "print(not_in_sw)\n",
    "print(in_sw + not_in_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_an = 0\n",
    "not_in_an = 0\n",
    "for i in range(anPostsByWords.shape[1]):\n",
    "    if i % 1000 == 0:\n",
    "            print((i / 90335) * 100)\n",
    "            print(\"in_an: \", in_an)\n",
    "            print(\"not_in_an: \", not_in_an)\n",
    "    if anPostsByWords[:,i].sum() < 10:\n",
    "        not_in_an += 1\n",
    "    else:\n",
    "        in_an += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_an)\n",
    "print(not_in_an)\n",
    "print(in_an + not_in_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "    if depPostsByWords[:,i].sum() < 10:\n",
    "        print(vocab_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocab_list:\n",
    "    if model.wv.vocab[word].count < 10:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "    if allPostsByWords[:,i].sum() < 10:\n",
    "        print(vocab_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "        print(cnt)\n",
    "    if depPostsByWords[:,i].sum() < 10:\n",
    "        cnt += 1\n",
    "        #print(vocab_list[i])\n",
    "print(\"cnt  \", cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnt = 0\n",
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "        print(cnt)\n",
    "    if swPostsByWords[:,i].sum() < 10:\n",
    "        cnt += 1\n",
    "        #print(vocab_list[i])\n",
    "print(\"cnt  \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "    if anPostsByWords[:,i].sum() < 10:\n",
    "        print(vocab_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in range(len(vocab_list)):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"PROGRESS: \", i/90335)\n",
    "        print(cnt)\n",
    "    if anPostsByWords[:,i].sum() < 10:\n",
    "        cnt += 1\n",
    "        #print(vocab_list[i])\n",
    "print(\"cnt  \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_list) - cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
